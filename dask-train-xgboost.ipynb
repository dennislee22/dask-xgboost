{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c01683c-48aa-4899-80cb-76ba40d7f08c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping addon with invalid or excluded ID: {'type': 'cmladdon', 'path': '/runtime-addons/cmladdon-2.0.49-b279', 'spec': '\\nenv:\\n  MLFLOW_TRACKING_URI: cml://localhost\\n  MLFLOW_REGISTRY_URI: cml://localhost\\n  PYTHONPATH: ${PYTHONPATH}:/opt/cmladdons/python/site-customize\\n  R_LIBS_SITE: ${R_LIBS_SITE}:/opt/cmladdons/r/libs\\npaths:\\n  - /opt/cmladdons', 'version': '', 'id': -1}\n",
      "Skipping addon with invalid or excluded ID: {'type': 'cmladdon', 'path': '/runtime-addons/cmladdon-2.0.49-b279', 'spec': '\\nenv:\\n  MLFLOW_TRACKING_URI: cml://localhost\\n  MLFLOW_REGISTRY_URI: cml://localhost\\n  PYTHONPATH: ${PYTHONPATH}:/opt/cmladdons/python/site-customize\\n  R_LIBS_SITE: ${R_LIBS_SITE}:/opt/cmladdons/r/libs\\npaths:\\n  - /opt/cmladdons', 'version': '', 'id': -1}\n",
      "\n",
      "Dask Diagnostic dashboard:\n",
      "https://4dow92pvmcytb8u1.cmlws5.apps.dlee5.cldr.example/\n"
     ]
    }
   ],
   "source": [
    "import dask\n",
    "import time\n",
    "\n",
    "import cml.workers_v1 as workers\n",
    "dask_scheduler = workers.launch_workers(\n",
    "    n=1,\n",
    "    cpu=2,\n",
    "    memory=8,\n",
    "    code=f\"!dask-scheduler --host 0.0.0.0 --dashboard-address 127.0.0.1:8090\",\n",
    ")\n",
    "\n",
    "# Wait for the scheduler to start.\n",
    "time.sleep(10)\n",
    "\n",
    "scheduler_workers = workers.list_workers()\n",
    "scheduler_id = dask_scheduler[0][\"id\"]\n",
    "scheduler_ip = [\n",
    "    worker[\"ip_address\"] for worker in scheduler_workers if worker[\"id\"] == scheduler_id\n",
    "][0]\n",
    "\n",
    "scheduler_url = f\"tcp://{scheduler_ip}:8786\"\n",
    "\n",
    "k8s_pods = 5\n",
    "dask_workers = workers.launch_workers(\n",
    "    n=k8s_pods,\n",
    "    cpu=1,\n",
    "    memory=8,\n",
    "    code=f\"!dask-worker {scheduler_url}\",\n",
    ")\n",
    "\n",
    "# Wait for the workers to start.\n",
    "time.sleep(10)\n",
    "\n",
    "print(\"\\nDask Diagnostic dashboard:\")\n",
    "print(\"//\".join(dask_scheduler[0][\"app_url\"].split(\"//\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e4ceb95-06b8-4c74-ba0a-f7a9ac685966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dask client connected: <Client: 'tcp://10.42.1.216:8786' processes=5 threads=160, memory=36.78 GiB>\n",
      "\n",
      "Reading '3G_cdr_data.csv' with Dask...\n",
      "Performing feature engineering with Dask...\n",
      "\n",
      "Training the XGBoost model with Dask...\n",
      "Calculating scale_pos_weight for class imbalance...\n",
      "scale_pos_weight: 18.96\n",
      "\n",
      "Model Evaluation on Test Set...\n",
      "[[45093     4]\n",
      " [    0  2352]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      1.00      1.00     45097\n",
      "        True       1.00      1.00      1.00      2352\n",
      "\n",
      "    accuracy                           1.00     47449\n",
      "   macro avg       1.00      1.00      1.00     47449\n",
      "weighted avg       1.00      1.00      1.00     47449\n",
      "\n",
      "\n",
      "Feature Importances:\n",
      "mobility                177.0\n",
      "total_calls             119.0\n",
      "avg_duration            100.0\n",
      "outgoing_call_ratio      85.0\n",
      "nocturnal_call_ratio     48.0\n",
      "std_duration              6.0\n",
      "dtype: float64\n",
      "\n",
      "Trained XGBoost model saved to 'fraud_detection_model_xgb.json'\n",
      "Process complete in 507.69 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Train xgboost model with dask\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client\n",
    "from xgboost.dask import DaskXGBClassifier\n",
    "from dask_ml.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import time\n",
    "\n",
    "def feature_engineering_dask(ddf):\n",
    "    print(\"Performing feature engineering with Dask...\")\n",
    "    ddf = ddf.set_index('msisdn') # ddf refers the input file to be declared in the '__main__' construct.\n",
    "\n",
    "    # New columns\n",
    "    ddf['is_outgoing'] = (ddf['call_direction'] == 'outgoing').astype(int) # 1 = call_direction is 'outgoing' and 0 otherwise.\n",
    "    ddf['is_nocturnal'] = ((ddf['hour_of_day'] >= 22) | (ddf['hour_of_day'] <= 6)).astype(int) # 1 if the hour_of_day is 10pm-6am.\n",
    "\n",
    "    # Declare new columns\n",
    "    meta = {\n",
    "        'total_calls': 'int64',\n",
    "        'outgoing_call_ratio': 'float64',\n",
    "        'avg_duration': 'float64',\n",
    "        'std_duration': 'float64',\n",
    "        'nocturnal_call_ratio': 'float64',\n",
    "        'mobility': 'int64',\n",
    "        'is_fraud': 'bool'\n",
    "    }\n",
    "\n",
    "    # groups the DataFrame by msisdn and applies the calculate_all_features_for_group function to each user's group of records.\n",
    "    user_features_ddf = ddf.groupby('msisdn').apply(\n",
    "        calculate_all_features_for_group,\n",
    "        meta=meta # Declare the new columns' data type\n",
    "    ).persist() # Keep the result in the RAM to avoid re-computing these features later.\n",
    "\n",
    "    return user_features_ddf\n",
    "\n",
    "def calculate_all_features_for_group(group):\n",
    "    group['is_fraud'] = group['is_fraud'].astype(bool)\n",
    "\n",
    "    nocturnal_hours = (group['hour_of_day'] >= 22) | (group['hour_of_day'] <= 6)\n",
    "    features = {\n",
    "        'total_calls': len(group), # The total number of records for that msisdn.\n",
    "        'outgoing_call_ratio': (group['call_direction'] == 'outgoing').mean(),\n",
    "        'avg_duration': group['duration'].mean(),\n",
    "        'std_duration': group['duration'].std(), # The standard deviation of the calls.\n",
    "        'nocturnal_call_ratio': nocturnal_hours.mean(), # The proportion of their calls made at night.\n",
    "        'mobility': group['cell_tower'].nunique(), # The number of unique cell towers they connected to, a measure of their movement.\n",
    "        'is_fraud': group['is_fraud'].iloc[0] # The fraud label for that msisdn\n",
    "    }\n",
    "    return pd.Series(features) # Returns the calculated features as a pandas Series, which Dask will then assemble into the new DataFrame.\n",
    "\n",
    "def train_fraud_detection_model_xgb_dask(features_ddf, client):\n",
    "    print(\"\\nTraining the XGBoost model with Dask...\") \n",
    "    features_ddf = features_ddf.fillna(0) # Fills any missing values (e.g., std_duration could be NaN if a user made only one call) with 0.\n",
    "\n",
    "    X = features_ddf.drop('is_fraud', axis=1) # axis=1 refers to the column. Drop entire column.\n",
    "    y = features_ddf['is_fraud'] # make prediction based on X features.\n",
    "\n",
    "    # Splits the data into a training set (80%) and a testing set (20%).\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, shuffle=True\n",
    "    )\n",
    "\n",
    "    # Handle class imbalance safely\n",
    "    print(\"Calculating scale_pos_weight for class imbalance...\")\n",
    "    class_counts = y_train.value_counts().compute() # tells Dask to perform the calculation and bring the result back to the main process.\n",
    "    neg_count = class_counts.get(False, 1) # Get count for label False\n",
    "    pos_count = class_counts.get(True, 1)  # Get count for label True\n",
    "    scale_pos_weight = neg_count / pos_count #Calculates the ratio of non-fraudulent to fraudulent msisdn. \n",
    "    print(f\"scale_pos_weight: {scale_pos_weight:.2f}\") #This value tells the model to pay more attention to the fraudulent cases.\n",
    "\n",
    "    model = DaskXGBClassifier(\n",
    "        n_estimators=100, # 100 trees\n",
    "        random_state=42, # ensures this randomness is the same every time code runs.\n",
    "        scale_pos_weight=scale_pos_weight, #count(negative)/count(positive), focus on fraudulent cases.\n",
    "        objective='binary:logistic', #binary classification and the model output the probability of a sample belonging to the positive class.\n",
    "        eval_metric='logloss', #report the logloss on a test set after each tree is built.\n",
    "        tree_method='hist' # find the best split points when building a tree.\n",
    "    )\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    print(\"\\nModel Evaluation on Test Set...\")\n",
    "    y_pred = model.predict(X_test) #lazy operation.\n",
    "    y_test_computed, y_pred_computed = client.compute([y_test, y_pred], sync=True)\n",
    "\n",
    "    print(confusion_matrix(y_test_computed, y_pred_computed))\n",
    "    print(classification_report(y_test_computed, y_pred_computed))\n",
    "\n",
    "    # Get booster and feature importances\n",
    "    booster = model.get_booster()\n",
    "    feature_scores = booster.get_score(importance_type='weight')\n",
    "    feature_importances = pd.Series(feature_scores).sort_values(ascending=False) # Sort items from the highest value to the lowest value.\n",
    "\n",
    "    print(\"\\nFeature Importances:\")\n",
    "    print(feature_importances)\n",
    "\n",
    "    return booster  # Return Booster object\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    client = Client(scheduler_url) # Connects to the Dask scheduler. scheduler_url is already declared outside of this script.\n",
    "    print(f\"Dask client connected: {client}\")\n",
    "\n",
    "    raw_data_filename = '3G_cdr_data.csv'\n",
    "    model_output_filename = 'fraud_detection_model_xgb.json'\n",
    "\n",
    "    try:\n",
    "        print(f\"\\nReading '{raw_data_filename}' with Dask...\")\n",
    "        raw_ddf = dd.read_csv(raw_data_filename, blocksize=\"128MB\") # tells Dask to partition the file into 128MB chunks. Default = 64MB.\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Raw data file not found at '{raw_data_filename}'.\")\n",
    "        print(\"Please run the data creation script first.\")\n",
    "        client.close() # Close connection to Dask cluster.\n",
    "        exit()\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    features_ddf = feature_engineering_dask(raw_ddf)\n",
    "    fraud_model_booster = train_fraud_detection_model_xgb_dask(features_ddf, client)\n",
    "\n",
    "    # Save the model in JSON format\n",
    "    fraud_model_booster.save_model(model_output_filename)\n",
    "\n",
    "    print(f\"\\nTrained XGBoost model saved to '{model_output_filename}'\")\n",
    "    print(f\"Process complete in {time.time() - start_time:.2f} seconds.\")\n",
    "    \n",
    "    client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0565cf-f3da-45c6-9d5b-f464235a3ba4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
